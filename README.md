# codingTasks
# Decision Tree on Titanic Dataset

# Description
This project involves using a Decision Tree classifier to predict the survival of passengers on the Titanic. The Titanic dataset is a well-known dataset in the machine learning community, and it provides an excellent opportunity to practice data preprocessing, feature engineering, and applying machine learning algorithms. Understanding decision trees is crucial for building interpretable models and is a foundation for more complex ensemble methods.

# Table of Contents
1. [Description](#description)
2. [Installation](#installation)
3. [Usage](#usage)
4. [Credits](#credits)

# Installation
To run this project locally, you need to have Python and Jupyter Notebook installed. Additionally, install the necessary packages using the following command:
1. Clone the repository:
    ```bash
    git clone https://github.com/JBurgess96/codingTasks.git
    ```

2. Navigate to the project directory:
    ```bash
    cd codingTasks/decision_tree_titanic
    ```
    
3. Install the necessary packages using the following command:
    ```bash
    pip install pandas numpy matplotlib scikit-learn seaborn
    ```
    
4. Run the Python script:
    ```bash
    python decision_tree_titanic.py
    ```


# Usage

Follow these steps to use the code:

Clone the Repository
```bash
git clone https://github.com/your-username/codingTasks.git
```

Navigate to the Project Directory:
```bash
cd codingTasks
```

Open the Jupyter Notebook:
```bash
jupyter notebook decision_tree_titanic.ipynb
```

Run the Notebook:

Open the decision_tree_titanic.ipynb notebook in Jupyter.
Run the cells sequentially to preprocess the data, train the decision tree model, and evaluate its performance.

The notebook includes steps to:
 - Load and preprocess the Titanic dataset.
 - Encode categorical variables using one-hot encoding.
 - Handle missing values.
 - Train a Decision Tree classifier.
 - Evaluate the model's performance using accuracy score.
   
Additionally, it demonstrates the use of ensemble methods such as Bagging, Random Forest, and Gradient Boosting classifiers, and includes hyperparameter tuning using GridSearchCV.

![screenshot1](https://github.com/Jburgess96/codingTasks/assets/163059737/c1a95cce-7fba-4749-adf7-a8d4bf56153f)
![screenshot2](https://github.com/Jburgess96/codingTasks/assets/163059737/0a5a6471-68e3-4d66-bbfa-3676725d9d05)


# Credits
Written by JBurgess96 - Jason Burgess
